正则化
===

- [Dropout](#dropout)
    - [常见问题](#常见问题)

## Dropout

**工作流程**
- 【训练阶段】前向传播时，对每个神经元以概率 $p$ 使其失活（即置为 `0`），而其他未失活的单元会乘以 $\frac{1}{1-p}$（放大）。
- 【测试阶段】使 dropout 失效，即正常使用所有神经元；

> **为什么要乘以 $\frac{1}{1-p}$？**
>> 如果一个神经元

> 以上是主流的实现方式，另一种是在训练时保持，测试时每个神经元乘以 $(1-p)$（缩小），但这种方式会增加预测时的计算量，所以一般不使用；

**作用**
- 提升泛化，减少过拟合；

**原理**
- Dropout 提供了一种廉价的 Bagging 集成近似；

**Pytorch**
```python
import torch.nn as nn

drouput = nn.Dropout(0.2)
x = drouput(x)
```

### 常见问题

- **Dropout 在训练和测试时有什么区别？**
    > 训练时，经过 Dropout 的输出值会乘以 $\frac{1}{1-p}$，测试时不会。
- **为什么 Dropout 在训练时要乘以 $\frac{1}{1-p}$？**
    > 还原神经元的输出；
    >> 如果一个激活值以概率 $p$ 置 0，那么它的期望输出将变为 $p*0 + (1-p)x=(1-p)x$，为了还原期望值 $x$，故乘以 $\frac{1}{1-p}$
- **为什么 Dropout 能防止过拟合？**
    > 直观上，Dropout 会使部分神经元失活，减小了模型容量，从而降低了模型的拟合能力；宏观上，Dropout 提供了一种廉价的 Bagging 集成方法（共享权重）；  

    > 其他说法：隐藏单元经过 Dropout 后，必须学习与不同采样的神经元合作，使得神经元具有更强的健壮性；